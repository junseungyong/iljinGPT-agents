import streamlit as st
from langchain_core.messages.chat import ChatMessage
from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_teddynote.prompts import load_prompt
from dotenv import load_dotenv
from langchain_teddynote import logging
from layout_parser import graph_document_ai, GraphState
import pprint

import os
from constants import PROGRESS_MESSAGE_GRAPH_NODES
from output import create_md, create_and_download_zip, clean_cache_files


# API KEY ì •ë³´ë¡œë“œ
load_dotenv()

# í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì…ë ¥í•©ë‹ˆë‹¤.
logging.langsmith("Document AI")

# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
if not os.path.exists(".cache"):
    os.mkdir(".cache")

# íŒŒì¼ ì—…ë¡œë“œ ì „ìš© í´ë”
if not os.path.exists(".cache/files"):
    os.mkdir(".cache/files")

# ì„ë² ë”© íŒŒì¼ ì €ì¥ í´ë”
if not os.path.exists(".cache/embeddings"):
    os.mkdir(".cache/embeddings")


st.title("Document AI ğŸ’¬")

# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ
if "messages" not in st.session_state:
    # ëŒ€í™”ê¸°ë¡ì„ ì €ì¥í•˜ê¸° ìœ„í•œ ìš©ë„ë¡œ ìƒì„±í•œë‹¤.
    st.session_state["document_messages"] = []

if "chain" not in st.session_state:
    # ì•„ë¬´ëŸ° íŒŒì¼ì„ ì—…ë¡œë“œ í•˜ì§€ ì•Šì„ ê²½ìš°
    st.session_state["chain"] = None

if "filepath" not in st.session_state:
    st.session_state["filepath"] = None


# ì‚¬ì´ë“œë°” ìƒì„±
with st.sidebar:
    # íŒŒì¼ ì—…ë¡œë“œ
    uploaded_files = st.file_uploader(
        "Upload a file",
        type=["pdf", "png", "jpg", "jpeg", "bmp", "tiff"],
        accept_multiple_files=True,
    )
    # ëª¨ë¸ ì„ íƒ ë©”ë‰´
    translate_lang = st.selectbox("Translate", ["Korean", "English", "German"], index=0)
    # Translate í† ê¸€ ì¶”ê°€
    translate_toggle = st.checkbox("Enable Translation", value=False)
    # AI Translate & Summary ë²„íŠ¼ ìƒì„±
    start_btn = st.button("Document AI")


def download_files(file_path, state):
    zip_filepath = os.path.splitext(file_path)[0]  # í´ë” ê²½ë¡œd

    if translate_toggle:
        create_md(file_path, state, "translate")

    create_md(file_path, state, "text_summary")
    create_md(file_path, state, "image_summary")
    create_md(file_path, state, "table_summary")

    zip_filename = create_and_download_zip(zip_filepath)  # ì „ì²´ ê²½ë¡œë¥¼ ë°›ìŒ

    # ì••ì¶• íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    if zip_filename and os.path.exists(
        zip_filename
    ):  # zip_filenameì´ Noneì´ ì•„ë‹ ë•Œ í™•ì¸
        with open(zip_filename, "rb") as f:
            st.download_button(
                label="Download Results",
                data=f,
                file_name=os.path.basename(zip_filename),  # íŒŒì¼ ì´ë¦„ë§Œ ì‚¬ìš©
                mime="application/zip",
            )

        # ì„ì‹œ ì••ì¶• íŒŒì¼ ì‚­ì œ
        os.remove(zip_filename)  # zip_filenameì„ ì‚­ì œ
    else:
        st.error("Problem in creating zip file")


# ì´ì „ ëŒ€í™”ë¥¼ ì¶œë ¥
def print_messages():
    for chat_message in st.session_state["document_messages"]:
        st.chat_message(chat_message.role).write(chat_message.content)


# ìƒˆë¡œìš´ ë©”ì‹œë¦¬ë¥¼ ì¶”ê°€
def add_message(role, message):
    st.session_state["document_messages"].append(
        ChatMessage(role=role, content=message)
    )


def create_chain(retriever):
    # ë‹¨ê³„ 6: í”„ë¡¬í”„íŠ¸ ìƒì„±(Create Prompt)
    prompt = load_prompt("prompts/pdf-rag.yaml", encoding="utf-8")

    # ë‹¨ê³„ 7: ì–¸ì–´ëª¨ë¸(LLM) ìƒì„±
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    # llm = ChatOllama(model="gemma2-27B:latest", temperature=0)

    # ë‹¨ê³„ 8: ì²´ì¸(Chain) ìƒì„±
    chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    return chain


# íŒŒì¼ì„ ìºì‹œ ì €ì¥(ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ëŠ” ì‘ì—…ì„ ì²˜ë¦¬í•  ì˜ˆì •)
def cache_file(files):
    file_extension = os.path.splitext(files[0].name)[
        1
    ].lower()  # ì²« ë²ˆì§¸ íŒŒì¼ì˜ í™•ì¥ì í™•ì¸
    # ì—…ë¡œë“œí•œ íŒŒì¼ì„ ìºì‹œ ë””ë™í† ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤.
    file_paths = []
    if files:  # íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°
        if file_extension == ".pdf":
            if len(files) > 1:  # íŒŒì¼ì´ 1ê°œ ì´ìƒì¸ ê²½ìš°
                st.warning("PDF file only support one file")
                st.session_state["filepath"] = None
            else:
                file_content = files[0].read()  # ì²« ë²ˆì§¸ íŒŒì¼ ì½ê¸°
                file_path = f"./.cache/files/{files[0].name}"
                with open(file_path, "wb") as f:
                    f.write(file_content)
                # ì¶”ê°€ì ì¸ PDF íŒŒì¼ ì²˜ë¦¬ ì½”ë“œ ì‘ì„± ê°€ëŠ¥
                file_paths.append(file_path)
                st.session_state["filepath"] = file_paths
        # elif file_extension in [".png", ".jpg", ".jpeg", ".bmp", ".tiff"]:
        #     for file in files:
        #         file_content = file.read()  # íŒŒì¼ ì½ê¸°
        #         file_path = f"./.cache/files/{file.name}"
        #         with open(file_path, "wb") as f:
        #             f.write(file_content)
        #         file_paths.append(file_path)
        #     st.session_state["filepath"] = file_paths
        else:
            st.warning("Not supported file type")
            st.session_state["filepath"] = None


if uploaded_files:
    # íŒŒì¼ ì—…ë¡œë“œ í›„ retriever ìƒì„± (ì‘ì—…ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ì˜ˆì •)
    cache_file(uploaded_files)
    uploaded_files = None


def process_graph(file_paths):
    print("process_graph")
    # ì§„í–‰ë„ í‘œì‹œë¥¼ ìœ„í•œ ì»¨í…Œì´ë„ˆ ìƒì„±
    progress_bar = st.progress(0)
    status_container = st.empty()

    state = GraphState()

    # ê·¸ë˜í”„ ìƒì„±
    message_dict = PROGRESS_MESSAGE_GRAPH_NODES
    graph = graph_document_ai(translate_toggle)
    inputs = {
        "filepath": file_paths[0],
        "batch_size": 10,
        "translate_lang": translate_lang,
        "translate_toggle": translate_toggle,
    }

    total_nodes = len(graph.nodes)

    for i, output in enumerate(graph.stream(inputs)):
        print(f"output: {output}")
        progress = (i + 1) / total_nodes
        progress_bar.progress(progress)

        # ì§„í–‰ë„ë¥¼ ë°±ë¶„ìœ¨ë¡œ ê³„ì‚°
        progress_percentage = int(progress * 100)

        for key, value in output.items():
            # ë‹¤ìŒ ë‹¨ê³„ì˜ ë©”ì‹œì§€ë¥¼ í‘œì‹œ
            pprint.pprint(f"output from node '{key}':")
            status_container.text(f"{progress_percentage}% - {message_dict[key]}")
            # pprint.pprint("---")
            # pprint.pprint(value, indent=2, width=80, depth=None)
            state.update(value)

    # Progress barë¥¼ 100%ë¡œ ì„¤ì •í•˜ê³  "Finished!" ë©”ì‹œì§€ë¥¼ ë…¹ìƒ‰ìœ¼ë¡œ í‘œì‹œ
    progress_bar.progress(1.0)
    status_container.markdown(":green[100% - Finished!]")

    return state


if start_btn:
    file_paths = st.session_state["filepath"]
    print(f"file_paths: {file_paths}")
    if file_paths is None:
        st.error("Please upload a file first.")
    else:
        state = process_graph(file_paths)
        download_files(file_paths[0], state)
        st.session_state["filepath"] = None
        # retriever = create_retriever_save_local(
        #     file_paths, state, "RBA_index", translate_toggle
        # )
        # if retriever:
        #     chain = create_chain(retriever)
        #     st.session_state["chain"] = chain
        clean_cache_files()


# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥
print_messages()

# ì‚¬ìš©ìì˜ ì…ë ¥
user_input = st.chat_input("Ask your question!")

# ê²½ê³  ë©”ì‹œì§€ë¥¼ ë„ìš°ê¸° ìœ„í•œ ë¹ˆ ì˜ì—­
warning_msg = st.empty()

if user_input:
    # chainì„ ìƒì„±
    chain = st.session_state["chain"]

    if chain is not None:
        # ì‚¬ìš©ìì˜ ì…ë ¥
        st.chat_message("user").write(user_input)
        # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
        response = chain.stream(user_input)
        with st.chat_message("assistant"):
            # ë¹ˆ ê³µê°„(ì»¨í…Œì´ë„ˆ)ë¥¼ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥í•œë‹¤.
            container = st.empty()

            ai_answer = ""
            for token in response:
                ai_answer += token
                container.markdown(ai_answer)

        # ëŒ€í™”ê¸°ë¡ì„ ì €ì¥
        add_message("user", user_input)
        add_message("assistant", ai_answer)
    else:
        warning_msg.error("Please upload a file first.")
